{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 台積電財報 RAG 系統專案報告 (整合 OCR 功能)\n",
        "\n",
        "## 專案簡介\n",
        "本專案旨在建立一個基於檢索增強生成 (Retrieval Augmented Generation, RAG) 技術的問答系統，能夠針對台積電的合併財務報告提供精準回答。為了解決 PDF 檔案中可能包含圖片形式的文字（如表格），本專案整合了 OCR (Optical Character Recognition) 技術，增強了系統從不同格式的文本中提取資訊的能力，進而改善問答的準確性。\n",
        "\n",
        "## 數據來源\n",
        "本專案使用的數據來源是從公開渠道獲取的**台積電 114 年及 113 年第一季合併財務報告**的 PDF 檔案 (`202501_2330_AI1_20250801_131448.pdf`)。\n"
      ],
      "metadata": {
        "id": "g8_OKpbYOu__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###掛載 Google Drive\n",
        "本專案會將處理後的數據和向量索引存儲在 Google Drive 中，以便在不同 Colab 工作階段中重複使用。"
      ],
      "metadata": {
        "id": "bwcTk3w_PkOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8KnRGPnFo1l"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8550535"
      },
      "source": [
        "## 安裝必要套件和軟體\n",
        "\n",
        "使用 PyMuPDF 和 pytesseract 結合進行 OCR，以提取包含圖片頁面的 PDF 文字，並將提取的文字用於改進 RAG 系統的檢索和回答能力。\n",
        "安裝 `pytesseract` Python 函式庫以及 Tesseract OCR 引擎本身。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e6c2582"
      },
      "source": [
        "# 安裝 Tesseract OCR 引擎\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install tesseract-ocr -y\n",
        "\n",
        "# 安裝 Tesseract 中文語言包\n",
        "!sudo apt-get install tesseract-ocr-chi-tra -y # 繁體中文\n",
        "\n",
        "# 安裝 pytesseract Python 函式庫\n",
        "!pip install pytesseract\n",
        "\n",
        "!pip install pymupdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 數據前處理\n",
        "數據前處理是 RAG 系統的關鍵步驟，旨在將非結構化的 PDF 內容轉換為可供檢索模型使用的結構化文本塊。\n",
        "\n",
        "**PDF 文本提取 (含 OCR)**:\n",
        "    *   使用 **PyMuPDF** 庫進行基礎文本提取。\n",
        "    *   為了處理圖片頁面或文字提取不足的頁面，結合 **pytesseract** 進行 OCR。\n",
        "    *   修改了提取函式 `extract_text_from_pdf_pymupdf_with_ocr`，當單頁直接提取的文字少於預設閾值 (`text_threshold=50`) 時，會將該頁渲染為圖片並使用 Tesseract OCR 進行文字辨識（使用繁體中文語言包 `chi_tra`），再將 OCR 結果添加到總文本中。\n",
        "    *   提取的文本被儲存到變數 `tsmc_financial_report_text_with_ocr`。"
      ],
      "metadata": {
        "id": "ZI74xcJHP4y8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e35035c"
      },
      "source": [
        "import fitz # 導入 PyMuPDF 庫\n",
        "import pytesseract\n",
        "from PIL import Image # 導入 Pillow 庫的 Image 模組\n",
        "import io # 導入 io 模組用於處理圖片數據\n",
        "\n",
        "def extract_text_from_pdf_pymupdf_with_ocr(pdf_file_path, text_threshold=50):\n",
        "    \"\"\"\n",
        "    使用 PyMuPDF 從 PDF 檔案中提取所有文字內容。\n",
        "    如果單頁提取的文字少於 text_threshold，則嘗試使用 OCR 從圖片中提取文字。\n",
        "    \"\"\"\n",
        "    text_content = \"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_file_path) # 開啟 PDF 文件\n",
        "        print(f\"成功開啟 PDF: {pdf_file_path}, 共 {doc.page_count} 頁\")\n",
        "\n",
        "        # 嘗試提取每一頁的文字\n",
        "        for page_num in range(doc.page_count):\n",
        "            try:\n",
        "                page = doc.load_page(page_num) # 載入頁面\n",
        "\n",
        "                # 優先嘗試直接提取文字\n",
        "                page_text = page.get_text()\n",
        "\n",
        "                # 檢查提取的文字量，如果很少，嘗試 OCR\n",
        "                # 移除空白字符後計算長度，忽略空頁或只有少量標點符號的頁面\n",
        "                if len(page_text.strip()) < text_threshold:\n",
        "                    print(f\"頁面 {page_num + 1} 直接提取文字量少 ({len(page_text.strip())} < {text_threshold})，嘗試 OCR...\")\n",
        "\n",
        "                    # 將頁面渲染為圖片\n",
        "                    # dpi 越高，圖片越清晰，OCR 效果可能更好，但也會佔用更多記憶體和時間\n",
        "                    pix = page.get_pixmap(dpi=300)\n",
        "                    img = Image.open(io.BytesIO(pix.tobytes(\"png\"))) # 將 pixmap 轉換為 PIL Image\n",
        "\n",
        "                    # 使用 Tesseract 進行 OCR\n",
        "                    # 使用繁體中文語言包\n",
        "                    ocr_text = pytesseract.image_to_string(img, lang='chi_tra')\n",
        "                    print(f\"頁面 {page_num + 1} OCR 提取文字量: {len(ocr_text.strip())}\")\n",
        "\n",
        "                    # 將 OCR 結果添加到總文本中\n",
        "                    text_content += ocr_text + \"\\n--- Page Separator (OCR) ---\\n\"\n",
        "\n",
        "                else:\n",
        "                    # 如果直接提取的文字足夠多，使用直接提取的文字\n",
        "                    text_content += page_text + \"\\n--- Page Separator (Text) ---\\n\"\n",
        "                    print(f\"已提取頁面 {page_num + 1} (直接提取)\")\n",
        "\n",
        "            except Exception as page_e:\n",
        "                print(f\"處理頁面 {page_num + 1} 時發生錯誤: {page_e}\")\n",
        "                text_content += f\"\\n--- Error extracting Page {page_num + 1} ---\\n\" # 標記提取失敗的頁面\n",
        "\n",
        "        doc.close() # 關閉文件\n",
        "        return text_content\n",
        "    except Exception as e:\n",
        "        print(f\"開啟或處理 PDF 時發生錯誤: {e}\")\n",
        "        return None\n",
        "\n",
        "pdf_file = \"/content/drive/MyDrive/financial_rag_project/data financial_reports/202501_2330_AI1_20250801_131448.pdf\"\n",
        "\n",
        "# 呼叫函式並將提取的文字內容儲存到變數中\n",
        "tsmc_financial_report_text_with_ocr = extract_text_from_pdf_pymupdf_with_ocr(pdf_file, text_threshold=50)\n",
        "\n",
        "if tsmc_financial_report_text_with_ocr:\n",
        "    print(\"\\n文字內容已成功使用 PyMuPDF + OCR 提取並儲存至變數 'tsmc_financial_report_text_with_ocr'。\")\n",
        "else:\n",
        "    print(\"未能提取 PDF 內容。請確認檔案是否存在且未損壞。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ce83664"
      },
      "source": [
        "# 將提取的文本儲存到檔案以便察看結果\n",
        "output_text_file = \"/content/extracted_tsmc_text.txt\"\n",
        "with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(tsmc_financial_report_text_with_ocr)\n",
        "print(f\"提取的文本已儲存至: {output_text_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083eda63"
      },
      "source": [
        "## 更新文字清洗和後續步驟\n",
        "\n",
        "**文字清洗**:\n",
        "    *   由於 OCR 結果可能包含雜訊、錯誤辨識字符或不規則格式，開發了 `clean_financial_text_with_ocr` 函式進行清洗。\n",
        "    *   清洗步驟包括：\n",
        "        *   移除頁碼標記（如 `- 1 -`）。\n",
        "        *   移除反斜線字符。\n",
        "        *   將多個換行符替換為單個換行符。\n",
        "        *   將多個空白字符替換為單個空格，並移除首尾空白。\n",
        "        *   移除 OCR 或文本提取過程中可能產生的頁面分隔符 (`--- Page Separator ---`) 和錯誤標記。\n",
        "    *   清洗後的文本儲存在 `cleaned_tsmc_text_with_ocr` 變數中，並保存到檔案 `/content/drive/MyDrive/FinancialRAGData/cleaned_tsmc_text_with_ocr.txt`。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_tuSrqci50_"
      },
      "source": [
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import json\n",
        "import os\n",
        "\n",
        "def clean_financial_text_with_ocr(text):\n",
        "    \"\"\"\n",
        "    Clean financial text extracted from PDF, potentially including OCR results.\n",
        "    Removes excess whitespace, line breaks, page number markers, OCR specific noise, etc.\n",
        "    \"\"\"\n",
        "    print(f\"--- Cleaning Text ---\")\n",
        "    print(f\"Initial text length: {len(text)}\")\n",
        "\n",
        "    # Remove page number markers like '- 1 -', '- 23 -', etc.\n",
        "    text = re.sub(r'- \\d+ -', '', text)\n",
        "    print(f\"After removing page numbers: {len(text)}\")\n",
        "\n",
        "    # Remove '\\' characters (if present from tools)\n",
        "    text = re.sub(r'\\\\', '', text)\n",
        "    print(f\"After removing backslashes: {len(text)}\")\n",
        "\n",
        "    # Replace multiple newline characters with a single newline\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "    print(f\"After normalizing newlines: {len(text)}\")\n",
        "\n",
        "    # Replace multiple whitespace characters with a single space, then strip leading/trailing whitespace\n",
        "    # Doing this early might help simplify subsequent steps\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    print(f\"After normalizing whitespace and strip: {len(text)}\")\n",
        "\n",
        "    # Remove \"--- Page Separator ---\" markers\n",
        "    text = re.sub(r'--- Page Separator \\(OCR\\) ---', '', text)\n",
        "    print(f\"After removing OCR separator: {len(text)}\")\n",
        "    text = re.sub(r'--- Page Separator \\(Text\\) ---', '', text)\n",
        "    print(f\"After removing Text separator: {len(text)}\")\n",
        "    text = re.sub(r'--- Error extracting Page \\d+ ---', '', text) # Remove error markers as well\n",
        "    print(f\"After removing error markers: {len(text)}\")\n",
        "\n",
        "\n",
        "    # After cleaning, re-normalize whitespace and strip again\n",
        "    # This step is important after removing various patterns\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    print(f\"Final text length after re-normalizing whitespace and strip: {len(text)}\")\n",
        "\n",
        "\n",
        "    return text\n",
        "\n",
        "if 'tsmc_financial_report_text_with_ocr' not in globals() or tsmc_financial_report_text_with_ocr is None or len(tsmc_financial_report_text_with_ocr.strip()) == 0:\n",
        "    print(\"錯誤: 原始或包含 OCR 結果的 PDF 文本變數 'tsmc_financial_report_text_with_ocr' 不存在、為 None 或為空。\")\n",
        "    print(\"請先運行 PDF 提取儲存格並確認其輸出顯示成功提取到文字。\")\n",
        "    cleaned_tsmc_text_with_ocr = None # Ensure variable is set to None if source text is missing or empty\n",
        "else:\n",
        "    cleaned_tsmc_text_with_ocr = clean_financial_text_with_ocr(tsmc_financial_report_text_with_ocr)\n",
        "\n",
        "    print(\"\\n--- Cleaned Text with OCR Example ---\")\n",
        "    print(f\"\\nCleaned text length: {len(cleaned_tsmc_text_with_ocr)} characters\")\n",
        "\n",
        "    # --- 新增：保存清洗後的文本到檔案 ---\n",
        "    output_dir = \"/content/drive/MyDrive/FinancialRAGData\" # 與 Streamlit 應用程式中設定的路徑一致\n",
        "    cleaned_text_path = os.path.join(output_dir, \"cleaned_tsmc_text_with_ocr.txt\")\n",
        "    os.makedirs(output_dir, exist_ok=True) # 確保目錄存在\n",
        "\n",
        "    try:\n",
        "        with open(cleaned_text_path, \"w\", encoding='utf-8') as f:\n",
        "            f.write(cleaned_tsmc_text_with_ocr)\n",
        "        print(f\"\\n清洗後的文本已保存至: {cleaned_text_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n保存清洗後的文本檔案失敗: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**文本切割 (Chunking)**:\n",
        "    *   使用 LangChain 的 `RecursiveCharacterTextSplitter` 將清洗後的文本切割成較小的、有重疊的文本塊 (chunks)。\n",
        "    *   參數設定為 `chunk_size=800` 和 `chunk_overlap=150`，這有助於保留上下文並確保關鍵資訊不會被分割在不同的塊中。\n",
        "    *   為每個文本塊添加元數據，包括來源資訊和唯一的 `chunk_id`。\n",
        "    *   切割後的文本塊儲存在 `all_processed_chunks_with_ocr` 列表中。"
      ],
      "metadata": {
        "id": "n8rXh1xISQi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if cleaned_tsmc_text_with_ocr is not None and len(cleaned_tsmc_text_with_ocr) > 0: # Only chunk if cleaned text is not empty\n",
        "    def chunk_text_with_metadata(text, source_info, chunk_size=800, chunk_overlap=150):\n",
        "        \"\"\"\n",
        "        使用 LangChain 的 RecursiveCharacterTextSplitter 進行文本切割，並為每個 chunk 添加元數據。\n",
        "        \"\"\"\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len, # 使用字元長度\n",
        "            add_start_index=True, # 添加 chunk 在原始文本中的起始索引\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.create_documents([text], metadatas=[{\"source\": source_info}])\n",
        "\n",
        "        formatted_chunks = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            formatted_chunks.append({\n",
        "                \"text\": chunk.page_content,\n",
        "                \"source\": chunk.metadata[\"source\"],\n",
        "                \"chunk_id\": f\"{source_info.replace(' ', '_')}_chunk_{i}\" # 自定義一個唯一ID\n",
        "            })\n",
        "        return formatted_chunks\n",
        "\n",
        "    tsmc_source_info = \"台積電114年第1季合併財務報告 (含OCR)\" # Update source info to reflect OCR inclusion\n",
        "\n",
        "    # Execute text chunking with the cleaned OCR text\n",
        "    all_processed_chunks_with_ocr = chunk_text_with_metadata(\n",
        "        cleaned_tsmc_text_with_ocr,\n",
        "        tsmc_source_info,\n",
        "        chunk_size=800, # Keep the same chunk size and overlap as previous successful runs\n",
        "        chunk_overlap=150\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Text Chunking Example (with OCR) ---\")\n",
        "    print(f\"Total chunks created: {len(all_processed_chunks_with_ocr)}\")\n",
        "    if len(all_processed_chunks_with_ocr) > 0: # Only try to print examples if chunks exist\n",
        "        print(\"First chunk example:\")\n",
        "        print(all_processed_chunks_with_ocr[0])\n",
        "        if len(all_processed_chunks_with_ocr) > 1: # Only print last if there's more than one\n",
        "             print(\"\\nLast chunk example:\")\n",
        "             print(all_processed_chunks_with_ocr[-1])\n",
        "        else:\n",
        "             print(\"\\nOnly one chunk created.\")\n",
        "\n",
        "else:\n",
        "    # If cleaning failed or resulted in empty text, set all_processed_chunks_with_ocr to None or empty list\n",
        "    all_processed_chunks_with_ocr = [] # Set to empty list instead of None to avoid TypeErrors later\n",
        "    print(\"\\n文本清洗後為空，跳過文本切割。\")"
      ],
      "metadata": {
        "id": "OpXvWhoSSNIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###文本嵌入與向量資料庫\n",
        "建立向量索引\n",
        "目的： 這是 RAG 系統的核心。我們需要將每個文本塊轉換成數值向量（嵌入），然後將這些向量存儲在一個高效能的向量資料庫 (FAISS) 中。這個資料庫能夠快速地搜尋與使用者問題最相似的文本塊。\n",
        "\n",
        "使用的技術：\n",
        "\n",
        "sentence-transformers： 我們使用 paraphrase-multilingual-MiniLM-L12-v2 這個多語言嵌入模型。它可以將中文文本轉換成高品質的語義向量。\n",
        "\n",
        "faiss： Meta 開發的一個高效能相似度搜尋庫，非常適合用於處理大量的向量資料。"
      ],
      "metadata": {
        "id": "VkFNCG8RSt5V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceCQmVEuilDa"
      },
      "source": [
        "!pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIqwtnkcsKOG"
      },
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "print(\"嵌入模型加載完成。\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"生成文本的嵌入向量。\"\"\"\n",
        "    # convert_to_tensor=False 返回 NumPy array\n",
        "    return embedding_model.encode(text, convert_to_tensor=False)\n",
        "\n",
        "# 步驟1：生成所有文本塊的嵌入\n",
        "print(\"\\n--- 生成所有文本塊的嵌入 ---\")\n",
        "all_embeddings = []\n",
        "# document_store 將用於儲存原始文本和元數據，以便從 FAISS 索引恢復\n",
        "document_store = {}\n",
        "\n",
        "# 使用包含 OCR 結果的文本塊變數 all_processed_chunks_with_ocr\n",
        "for i, chunk_info in enumerate(all_processed_chunks_with_ocr):\n",
        "    embedding = get_embedding(chunk_info[\"text\"])\n",
        "    all_embeddings.append(embedding)\n",
        "    # 將 chunk 的所有資訊儲存起來，索引就是它在 FAISS 中的 ID\n",
        "    document_store[i] = chunk_info\n",
        "\n",
        "all_embeddings_np = np.array(all_embeddings).astype('float32') # FAISS 需要 float32 類型\n",
        "\n",
        "# 步驟2：建立 FAISS 索引\n",
        "print(\"\\n--- 建立 FAISS 索引 ---\")\n",
        "dimension = all_embeddings_np.shape[1] # 嵌入向量的維度\n",
        "index = faiss.IndexFlatL2(dimension) # 使用 L2 距離進行搜索 (歐幾里得距離)\n",
        "index.add(all_embeddings_np) # 將所有嵌入添加到索引中\n",
        "\n",
        "print(f\"FAISS 索引建立完成，共 {index.ntotal} 個向量 (文本塊)。\")\n",
        "\n",
        "# 步驟3：儲存 FAISS 索引和 document_store (方便下次直接加載，無需重複計算)\n",
        "\n",
        "# 確保輸出目錄存在\n",
        "output_dir = \"/content/drive/MyDrive/FinancialRAGData\" # 修改為您 Google Drive 中的路徑，使用一個目錄來存放檔案\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "faiss_index_path = os.path.join(output_dir, \"tsmc_financial_docs.faiss\")\n",
        "document_store_path = os.path.join(output_dir, \"tsmc_document_store.json\")\n",
        "\n",
        "faiss.write_index(index, faiss_index_path)\n",
        "with open(document_store_path, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(document_store, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"FAISS 索引已保存至: {faiss_index_path}\")\n",
        "print(f\"文件資料庫已保存至: {document_store_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###大型語言模型 (LLM) 整合\n",
        "使用 Google Gemini Pro API\n",
        "目的： 整合 Google Gemini Pro 模型，用於根據檢索到的上下文和使用者問題生成最終答案。我們選擇 Gemini API，是為了避免在 Colab CPU 環境下運行大型模型的效能瓶頸。\n",
        "\n",
        "使用的技術：\n",
        "\n",
        "google.generativeai： Google 提供的 SDK，用於輕鬆與 Gemini API 進行互動。\n",
        "\n",
        "Colab Secrets： 用於安全地存儲 API 金鑰，避免將敏感資訊硬編碼在筆記本中。"
      ],
      "metadata": {
        "id": "oZqpCud6TH4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"請在 Colab 的 Secrets 中設定 'GEMINI_API_KEY'！\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# --- 初始化 Gemini 模型 ---\n",
        "print(\"\\n--- 初始化 Google Gemini Pro 模型 ---\")\n",
        "llm_model = genai.GenerativeModel('models/gemini-1.5-flash-latest')\n",
        "print(\"Gemini Pro 模型初始化完成。\")\n",
        "\n",
        "# --- 定義 RAG 的 Prompt 模板 ---\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "您是一位金融分析師，請根據提供的以下金融文件內容，簡潔、專業地回答問題。\n",
        "如果文件內容沒有足夠的資訊來回答問題，請明確指出「文件內容不足以回答此問題」。\n",
        "\n",
        "文件內容：\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "問題：\n",
        "{question}\n",
        "\n",
        "回答：\n",
        "\"\"\"\n",
        "\n",
        "def generate_answer(query_text):\n",
        "    print(f\"\\n--- 處理問題 (使用 Gemini Pro): {query_text} ---\")\n",
        "\n",
        "    # 步驟1：檢索相關文件\n",
        "    retrieved_docs_info = retrieve_documents(query_text)\n",
        "\n",
        "    if not retrieved_docs_info:\n",
        "        print(\"未檢索到相關文件。\")\n",
        "        return \"對不起，我沒有找到相關的金融文件來回答這個問題。\", []\n",
        "\n",
        "    # 步驟2：組合上下文\n",
        "    context_texts = [doc[\"text\"] for doc in retrieved_docs_info]\n",
        "    context = \"\\n\\n\".join(context_texts)\n",
        "    print(f\"--- 檢索到的上下文 ({len(context_texts)} 條) ---\")\n",
        "    print(context[:500] + \"...\") # 打印部分上下文預覽\n",
        "\n",
        "    # 步驟3：格式化 Prompt\n",
        "    full_prompt = PROMPT_TEMPLATE.format(context=context, question=query_text)\n",
        "\n",
        "    # 步驟4：使用 Gemini API 生成答案\n",
        "    try:\n",
        "        # 調用 Gemini Pro 模型生成內容\n",
        "        response = llm_model.generate_content(full_prompt)\n",
        "\n",
        "        if hasattr(response, 'text'):\n",
        "            generated_answer = response.text.strip()\n",
        "        elif response.candidates: # 檢查是否有候選答案\n",
        "            generated_answer = response.candidates[0].content.parts[0].text.strip()\n",
        "        else:\n",
        "            generated_answer = \"Gemini 模型未能生成答案，可能因安全策略或其他內部錯誤。\"\n",
        "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n",
        "                print(f\"Prompt 被阻擋，原因: {response.prompt_feedback.block_reason.name}\")\n",
        "\n",
        "        print(f\"--- Gemini Pro 生成的答案 ---\")\n",
        "        print(generated_answer)\n",
        "\n",
        "        return generated_answer, retrieved_docs_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gemini API 調用發生錯誤: {e}\")\n",
        "        return \"生成答案時發生錯誤，請檢查 API 金鑰、網路連接或 Prompt 內容。\", []"
      ],
      "metadata": {
        "id": "f3RubQdE4N4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###建構檢索增強生成 (RAG) 管道與測試\n",
        "整合所有功能\n",
        "目的： 將之前建立的檢索功能和 LLM 生成功能組合成一個完整的 RAG 管道。這一步將是整個智慧問答系統的最終實現。"
      ],
      "metadata": {
        "id": "-SKIKPccTUFb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a0f0bbb"
      },
      "source": [
        "import re # 導入正規表達式模組\n",
        "\n",
        "# 檢索功能 (包含混合檢索邏輯)\n",
        "def retrieve_documents_with_ocr_index(query_text, top_k=10, cleaned_full_text=None):\n",
        "    \"\"\"\n",
        "    根據查詢文本，從包含 OCR 數據的 FAISS 索引中檢索最相關的文件塊。\n",
        "    對於特定的查詢，會嘗試基於關鍵詞的精確匹配。\n",
        "    \"\"\"\n",
        "\n",
        "    # 優先檢查是否為股票代碼查詢，並嘗試關鍵詞搜索\n",
        "    if \"股票代碼\" in query_text or \"股票號碼\" in query_text or \"證券代碼\" in query_text:\n",
        "        print(\"檢測到股票代碼相關查詢，嘗試關鍵詞搜索...\")\n",
        "        if cleaned_full_text:\n",
        "            # 使用正規表達式搜索 \"股票代碼：\" 後面的數字 (通常是4位)\n",
        "            match = re.search(r\"股票代碼[：:]\\s*(\\d+)\", cleaned_full_text)\n",
        "            if match:\n",
        "                stock_code_snippet = match.group(0) # 獲取匹配到的完整片段，例如 \"股票代碼： 2330\"\n",
        "                print(f\"關鍵詞搜索找到匹配: {stock_code_snippet}\")\n",
        "                # 構建一個模擬的文本塊列表，包含找到的片段和來源資訊\n",
        "                simulated_chunk = {\n",
        "                    \"text\": stock_code_snippet,\n",
        "                    \"source\": \"文件內容 (股票代碼關鍵詞匹配)\",\n",
        "                    \"chunk_id\": \"keyword_match_stock_code\"\n",
        "                }\n",
        "                return [simulated_chunk]\n",
        "            else:\n",
        "                print(\"股票代碼關鍵詞搜索未找到匹配，繼續...\")\n",
        "        else:\n",
        "            print(\"未提供完整的清洗後文本，無法執行股票代碼關鍵詞搜索，繼續...\")\n",
        "\n",
        "    # 檢查是否為營收相關查詢，並嘗試更複雜的關鍵詞搜索\n",
        "    # 尋找包含「營收」或「合併營收」以及「114年」和「第1季」或「第一季」的模式\n",
        "    revenue_pattern = re.compile(r\".*?(合併營收|營收).*?(114年).*?(第1季|第一季).*?(\\d{1,3}(,\\d{3})*(\\.\\d+)?).*?\", re.IGNORECASE)\n",
        "    if (\"營收\" in query_text or \"合併營收\" in query_text) and (\"114年\" in query_text or \"114 年\" in query_text) and (\"第一季\" in query_text or \"第1季\" in query_text or \"第 1 季\" in query_text):\n",
        "         print(\"檢測到114年第一季營收相關查詢，嘗試更複雜的關鍵詞搜索...\")\n",
        "         if cleaned_full_text:\n",
        "             # 在清洗後的文本中搜索匹配的模式\n",
        "             # 使用 finditer 找到所有匹配項\n",
        "             matches = list(revenue_pattern.finditer(cleaned_full_text))\n",
        "             if matches:\n",
        "                 revenue_snippets = []\n",
        "                 for match in matches:\n",
        "                     # 提取匹配到的完整片段，或者根據需要提取特定組 (如數字)\n",
        "                     snippet = match.group(0).strip()\n",
        "                     print(f\"營收關鍵詞搜索找到匹配: {snippet[:100]}...\")\n",
        "                     revenue_snippets.append({\n",
        "                         \"text\": snippet,\n",
        "                         \"source\": \"文件內容 (營收關鍵詞匹配)\",\n",
        "                         \"chunk_id\": f\"keyword_match_revenue_{matches.index(match)}\"\n",
        "                     })\n",
        "                 return revenue_snippets\n",
        "             else:\n",
        "                 print(\"營收關鍵詞搜索未找到匹配，回退到語義搜索。\")\n",
        "         else:\n",
        "             print(\"未提供完整的清洗後文本，無法執行營收關鍵詞搜索，回退到語義搜索。\")\n",
        "\n",
        "\n",
        "    # 如果不是上述特定查詢，或者關鍵詞搜索未找到結果，執行語義搜索\n",
        "    print(\"執行語義搜索...\")\n",
        "    if 'get_embedding' not in globals():\n",
        "        print(\"錯誤: get_embedding 函式未定義。請確保運行了加載嵌入模型的儲存格。\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = get_embedding(query_text).astype('float32')\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "    if 'index_with_ocr' not in globals() or 'document_store_with_ocr' not in globals():\n",
        "         print(\"錯誤: FAISS 索引或文件資料庫未載入。請確保運行了建立或載入索引的儲存格。\")\n",
        "         return []\n",
        "\n",
        "\n",
        "    distances, indices = index_with_ocr.search(query_embedding, top_k)\n",
        "\n",
        "    retrieved_docs_info = []\n",
        "    for i in indices[0]:\n",
        "        if i >= 0 and i < len(document_store_with_ocr):\n",
        "            doc_info = document_store_with_ocr[i]\n",
        "            retrieved_docs_info.append(doc_info)\n",
        "        else:\n",
        "             print(f\"警告: 檢索到無效索引 {i}。\")\n",
        "\n",
        "\n",
        "    return retrieved_docs_info # 返回包含 text 和 source 的字典列表\n",
        "\n",
        "def generate_answer_with_ocr_data(query_text):\n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\"**問題:** {query_text}\")\n",
        "    print(f\"--- 處理問題 (使用 Gemini Pro + OCR 數據): {query_text} ---\")\n",
        "\n",
        "    # 步驟1：檢索相關文件 (使用新的檢索函數，並傳入完整的清洗後文本)\n",
        "    if 'cleaned_tsmc_text_with_ocr' not in globals():\n",
        "        print(\"錯誤: cleaned_tsmc_text_with_ocr 未定義。請確保運行了文字清洗儲存格。\")\n",
        "        return \"處理錯誤：無法找到清洗後的文本。\", []\n",
        "\n",
        "    retrieved_docs_info = retrieve_documents_with_ocr_index(\n",
        "        query_text,\n",
        "        top_k=10,\n",
        "        cleaned_full_text=cleaned_tsmc_text_with_ocr\n",
        "    )\n",
        "\n",
        "    if not retrieved_docs_info:\n",
        "        print(\"未檢索到相關文件。\")\n",
        "        print(\"\\n**AI 的回答:**\")\n",
        "        print(\"對不起，我沒有找到相關的金融文件來回答這個問題。\")\n",
        "        print(\"**沒有找到直接相關的參考文件。**\")\n",
        "        print(f\"=======================================================\\n\")\n",
        "        return \"對不起，我沒有找到相關的金融文件來回答這個問題。\", []\n",
        "\n",
        "    # 步驟2：組合上下文\n",
        "    context_texts = [doc[\"text\"] for doc in retrieved_docs_info]\n",
        "    context = \"\\n\\n\".join(context_texts)\n",
        "    print(f\"--- 檢索到的上下文 ({len(retrieved_docs_info)} 條) ---\")\n",
        "    for i, text in enumerate(context_texts):\n",
        "        print(f\"Chunk {i+1} (Source: {retrieved_docs_info[i]['source']}): {text[:200]}...\")\n",
        "        if len(text) > 200:\n",
        "            print(\"...\")\n",
        "\n",
        "\n",
        "    # 步驟3：格式化 Prompt (確保 PROMPT_TEMPLATE 在使用前被定義)\n",
        "    # 將 PROMPT_TEMPLATE 的定義移到條件判斷之外，確保它總是作為局部變數被賦值\n",
        "    PROMPT_TEMPLATE = \"\"\"\n",
        "您是一位金融分析師，請根據提供的以下金融文件內容，簡潔、專業地回答問題。\n",
        "如果文件內容沒有足夠的資訊來回答問題，請明確指出「文件內容不足以回答此問題」。\n",
        "\n",
        "文件內容：\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "問題：\n",
        "{question}\n",
        "\n",
        "回答：\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    full_prompt = PROMPT_TEMPLATE.format(context=context, question=query_text)\n",
        "\n",
        "    # 步驟4：使用 Gemini API 生成答案 (確保 llm_model 已初始化)\n",
        "    if 'llm_model' not in globals():\n",
        "        print(\"Gemini model (llm_model) not initialized. Please run the initialization cell first.\")\n",
        "        print(\"\\n**AI 的回答:**\")\n",
        "        print(\"Gemini 模型未初始化，無法生成答案。\")\n",
        "        print(\"**沒有找到直接相關的參考文件。**\")\n",
        "        print(f\"=======================================================\\n\")\n",
        "        return \"Gemini 模型未初始化，無法生成答案。\", []\n",
        "\n",
        "    try:\n",
        "        response = llm_model.generate_content(full_prompt)\n",
        "\n",
        "        if hasattr(response, 'text'):\n",
        "            generated_answer = response.text.strip()\n",
        "        elif response.candidates:\n",
        "            generated_answer = \"\".join([part.text for part in response.candidates[0].content.parts]).strip()\n",
        "        else:\n",
        "            generated_answer = \"Gemini 模型未能生成答案，可能因安全策略或其他內部錯誤。\"\n",
        "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n",
        "                print(f\"Prompt 被阻擋，原因: {response.prompt_feedback.block_reason.name}\")\n",
        "\n",
        "        print(f\"\\n**AI 的回答:**\")\n",
        "        print(generated_answer)\n",
        "\n",
        "        if retrieved_docs_info:\n",
        "            print(\"\\n**參考來源 (部分文本與來源):**\")\n",
        "            for i, source_info in enumerate(retrieved_docs_info):\n",
        "                print(f\"  來源 {i+1}: {source_info['source']}\")\n",
        "                print(f\"    內容摘要: {source_info['text'][:100]}...\")\n",
        "        else:\n",
        "            print(\"\\n**沒有找到直接相關的參考文件。**\")\n",
        "\n",
        "        print(f\"=======================================================\\n\")\n",
        "\n",
        "\n",
        "        return generated_answer, retrieved_docs_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gemini API 調用發生錯誤: {e}\")\n",
        "        print(\"\\n**AI 的回答:**\")\n",
        "        print(f\"生成答案時發生錯誤: {e}\")\n",
        "        print(\"**沒有找到直接相關的參考文件。**\")\n",
        "        print(f\"=======================================================\\n\")\n",
        "        return \"生成答案時發生錯誤，請檢查 API 金鑰、網路連接或 Prompt 內容。\", []\n",
        "\n",
        "# 範例測試問題\n",
        "# test_queries = [\n",
        "#     \"台積電114年第一季的合併營收是多少？\",\n",
        "#     \"台積電的會計師核閱報告是由哪家會計師事務所出具的？\",\n",
        "#     \"台積電在大陸的投資資訊有什麼？\",\n",
        "#     \"台積電的股票代碼是什麼？\",\n",
        "#     \"台積電有哪些子公司?\"\n",
        "#     \"蘋果公司最近推出了什麼新產品？\" 這個問題應該無法回答，因為不在文件內容中"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "600e4c72"
      },
      "source": [
        "## 安裝 Streamlit 和 ngrok\n",
        "安裝 Streamlit 函式庫和 ngrok 工具（或 pyngrok）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d85549d3"
      },
      "source": [
        "# 安裝 Streamlit 和 pyngrok\n",
        "!pip install streamlit pyngrok\n",
        "\n",
        "import os\n",
        "if not os.path.exists('/usr/local/bin/ngrok'):\n",
        "    print(\"ngrok 執行檔未找到，正在下載並安裝...\")\n",
        "    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "    !unzip ngrok-stable-linux-amd64.zip\n",
        "    !sudo mv ngrok /usr/local/bin/\n",
        "    print(\"ngrok 安裝完成。\")\n",
        "else:\n",
        "    print(\"ngrok 執行檔已存在。\")\n",
        "\n",
        "# 驗證 ngrok 版本\n",
        "!ngrok --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamlit 應用\n",
        "為了提供一個友好的使用者介面來測試和展示 RAG 系統，本專案使用 Streamlit 建立了一個簡單的 Web 應用程式 (`app.py`)。\n",
        "\n",
        "*   應用程式載入預先保存的嵌入模型、FAISS 索引和文件資料庫。\n",
        "*   使用 `st.cache_resource` 快取模型和索引等資源，避免重複載入。\n",
        "*   提供一個文本輸入框供用戶輸入問題。\n",
        "*   點擊按鈕後，應用程式調用 RAG 邏輯（檢索相關文本並使用 Gemini 生成答案）。\n",
        "*   在介面中顯示 AI 的回答以及作為參考的來源文本塊和來源資訊。\n",
        "*   為了在 Colab 環境中運行 Streamlit 應用並通過公共 URL 訪問，使用了 **ngrok** (通過 `pyngrok` 庫) 建立 HTTP 通道。需要配置 ngrok Authtoken。Gemini API 金鑰通過臨時檔案安全地傳遞給 Streamlit 應用。\n"
      ],
      "metadata": {
        "id": "QIT16GG1T5_M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfb7f454"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "\n",
        "st.title('TSMC 財報 RAG 系統')\n",
        "st.write('請輸入關於**台積電114及113年第一季財報**的相關問題：')\n",
        "\n",
        "# --- RAG 組件載入 (使用 Streamlit 快取) ---\n",
        "@st.cache_resource\n",
        "def load_rag_components():\n",
        "    device = \"cpu\"\n",
        "    st.write(f\"模型將在設備: {device} 上加載和運行。\")\n",
        "\n",
        "    embedding_model = SentenceTransformer('bert-base-multilingual-cased', device=device)\n",
        "    st.write(\"嵌入模型加載完成。\")\n",
        "\n",
        "    output_dir = \"/content/drive/MyDrive/FinancialRAGData\"\n",
        "    faiss_index_path_with_ocr = os.path.join(output_dir, \"tsmc_financial_docs_with_ocr.faiss\")\n",
        "    document_store_path_with_ocr = os.path.join(output_dir, \"tsmc_document_store_with_ocr.json\")\n",
        "    cleaned_text_path = os.path.join(output_dir, \"cleaned_tsmc_text_with_ocr.txt\")\n",
        "\n",
        "    # 載入 FAISS 索引\n",
        "    st.write(f\"--- 載入 FAISS 索引: {faiss_index_path_with_ocr} ---\")\n",
        "    if not os.path.exists(faiss_index_path_with_ocr):\n",
        "        st.error(f\"錯誤: FAISS 索引檔案不存在於 {faiss_index_path_with_ocr}\")\n",
        "        return None, None, None, None, None\n",
        "    index_with_ocr = faiss.read_index(faiss_index_path_with_ocr)\n",
        "\n",
        "    # 載入文件資料庫\n",
        "    if not os.path.exists(document_store_path_with_ocr):\n",
        "        st.error(f\"錯誤: 文件資料庫檔案不存在於 {document_store_path_with_ocr}\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    with open(document_store_path_with_ocr, \"r\", encoding='utf-8') as f:\n",
        "        document_store_with_ocr = {int(k): v for k, v in json.load(f).items()}\n",
        "    st.write(\"文件資料庫載入完成。\")\n",
        "\n",
        "    # 載入清洗後的文本 (用於關鍵詞搜索的回退或輔助)\n",
        "    if not os.path.exists(cleaned_text_path):\n",
        "         st.error(f\"錯誤: 清洗後的文本檔案不存在於 {cleaned_text_path}\")\n",
        "         st.warning(\"Consider saving the cleaned text to a file after the cleaning step in the Colab notebook.\")\n",
        "         cleaned_full_text = None\n",
        "    else:\n",
        "        with open(cleaned_text_path, \"r\", encoding='utf-8') as f:\n",
        "            cleaned_full_text = f.read()\n",
        "\n",
        "\n",
        "    # 初始化 Gemini 模型\n",
        "    API_KEY = None\n",
        "    temp_api_key_files = glob.glob(os.path.join(output_dir, \".gemini_api_key_*.tmp\"))\n",
        "    if temp_api_key_files:\n",
        "        temp_api_key_path = temp_api_key_files[0]\n",
        "        try:\n",
        "            with open(temp_api_key_path, \"r\") as f:\n",
        "                API_KEY = f.read().strip()\n",
        "        except Exception as e:\n",
        "             st.error(f\"從臨時檔案讀取 GEMINI_API_KEY 失敗: {e}\")\n",
        "\n",
        "\n",
        "    if not API_KEY:\n",
        "         API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "\n",
        "    if not API_KEY:\n",
        "        st.error(\"請設定 'GEMINI_API_KEY' (在 Colab Secrets 並由筆記本寫入臨時檔案，或在部署環境中設定環境變數)！\")\n",
        "        llm_model = None\n",
        "    else:\n",
        "        try:\n",
        "            genai.configure(api_key=API_KEY)\n",
        "            llm_model = genai.GenerativeModel('models/gemini-1.5-flash-latest')\n",
        "        except Exception as e:\n",
        "            st.error(f\"Gemini API 初始化失敗: {e}\")\n",
        "            if \"'NoneType' object has no attribute 'kernel'\" in str(e):\n",
        "                 st.error(\"無法使用提供的 API 金鑰初始化 Gemini 模型。請確認金鑰有效。\")\n",
        "            llm_model = None\n",
        "\n",
        "\n",
        "    return embedding_model, index_with_ocr, document_store_with_ocr, llm_model, cleaned_full_text\n",
        "\n",
        "embedding_model, index_with_ocr, document_store_with_ocr, llm_model, cleaned_full_text = load_rag_components()\n",
        "\n",
        "# 檢查核心組件是否成功載入 (嵌入模型、索引、文件庫和語言模型)\n",
        "if embedding_model is None or index_with_ocr is None or document_store_with_ocr is None or llm_model is None:\n",
        "    st.error(\"核心 RAG 組件載入失敗。請檢查 Colab Secrets、Google Drive 檔案路徑和檔案是否存在，並確保 API 金鑰有效。\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    if embedding_model is None:\n",
        "         st.error(\"嵌入模型未載入，無法生成嵌入向量。\")\n",
        "         return None\n",
        "\n",
        "    return embedding_model.encode(text, convert_to_tensor=False)\n",
        "\n",
        "\n",
        "def retrieve_documents_optimized(query_text, top_k_semantic=10, top_k_keyword=5):\n",
        "    \"\"\"\n",
        "    優化後的混合檢索函數：\n",
        "    1. 先進行語義搜索，獲取 Top K 個相關文本塊。\n",
        "    2. 對特定問題類型進行關鍵詞搜索 (可在語義結果中或全文)。\n",
        "    3. 返回語義搜索結果和/或關鍵詞搜索結果。\n",
        "    \"\"\"\n",
        "    st.info(\"檢索策略: 語義搜索優先，並對特定問題進行關鍵詞搜索。\")\n",
        "\n",
        "    # 步驟1：執行語義搜索\n",
        "    query_embedding = get_embedding(query_text)\n",
        "    if query_embedding is None:\n",
        "        st.error(\"無法生成查詢嵌入向量，跳過檢索。\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = query_embedding.astype('float32')\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "    # 使用載入的索引 index_with_ocr 進行搜索\n",
        "    if index_with_ocr is None:\n",
        "        st.error(\"FAISS 索引未載入，無法執行搜索。\")\n",
        "        return []\n",
        "\n",
        "    distances, indices = index_with_ocr.search(query_embedding, top_k_semantic)\n",
        "\n",
        "    semantic_docs_info = []\n",
        "    if document_store_with_ocr is None:\n",
        "        st.error(\"文件資料庫未載入，無法檢索文檔信息。\")\n",
        "        return []\n",
        "\n",
        "    for i in indices[0]:\n",
        "        if i >= 0 and i in document_store_with_ocr:\n",
        "            doc_info = document_store_with_ocr[i]\n",
        "            semantic_docs_info.append(doc_info)\n",
        "        else:\n",
        "             st.warning(f\"警告: 語義檢索到無效索引 {i}。\")\n",
        "\n",
        "\n",
        "    # 步驟2：對特定問題類型進行關鍵詞搜索\n",
        "    keyword_docs_info = []\n",
        "\n",
        "    # 營收相關查詢的關鍵詞模式\n",
        "    revenue_pattern = re.compile(r\".*?(合併營收|營收).*?(114年).*?(第1季|第一季).*?(\\d{1,3}(,\\d{3})*(\\.\\d+)?).*?\", re.IGNORECASE)\n",
        "\n",
        "    # 股票代碼相關查詢的關鍵詞模式 (更簡單，只找數字)\n",
        "    stock_code_pattern = re.compile(r\"股票代碼[：:]\\s*(\\d+)\", re.IGNORECASE)\n",
        "\n",
        "    # 新增：會計師事務所相關查詢的關鍵詞模式\n",
        "    # 搜索包含 \"會計師\" 和 \"事務所\" 的片段\n",
        "    accounting_firm_pattern = re.compile(r\".*?會計師.*?事務所.*?\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "    # 判斷是否需要執行關鍵詞搜索，以及針對哪種模式\n",
        "    target_pattern = None\n",
        "    pattern_type = None\n",
        "\n",
        "    if (\"營收\" in query_text or \"合併營收\" in query_text) and (\"114年\" in query_text or \"114 年\" in query_text) and (\"第一季\" in query_text or \"第1季\" in query_text or \"第 1 季\" in query_text):\n",
        "        st.info(\"檢測到114年第一季營收相關查詢，嘗試關鍵詞搜索...\")\n",
        "        target_pattern = revenue_pattern\n",
        "        pattern_type = \"revenue\"\n",
        "        search_in_semantic_results = True\n",
        "        search_in_full_text = False\n",
        "    elif \"股票代碼\" in query_text or \"股票號碼\" in query_text or \"證券代碼\" in query_text:\n",
        "        st.info(\"檢測到股票代碼相關查詢，嘗試關鍵詞搜索...\")\n",
        "        target_pattern = stock_code_pattern\n",
        "        pattern_type = \"stock_code\"\n",
        "        search_in_semantic_results = False\n",
        "        search_in_full_text = True\n",
        "    # 新增針對會計師事務所的判斷\n",
        "    elif (\"會計師\" in query_text or \"事務所\" in query_text or \"核閱報告\" in query_text or \"查核報告\" in query_text):\n",
        "        st.info(\"檢測到會計師事務所相關查詢，嘗試關鍵詞搜索...\")\n",
        "        target_pattern = accounting_firm_pattern\n",
        "        pattern_type = \"accounting_firm\"\n",
        "        search_in_semantic_results = False\n",
        "        search_in_full_text = True\n",
        "\n",
        "\n",
        "    if target_pattern:\n",
        "        if search_in_full_text and cleaned_full_text:\n",
        "            st.info(\"在清洗後的全文中執行關鍵詞搜索...\")\n",
        "            matches = list(target_pattern.finditer(cleaned_full_text))\n",
        "            for match in matches:\n",
        "                snippet = match.group(0).strip()\n",
        "                start_index = match.start()\n",
        "                end_index = match.end()\n",
        "                context_start = max(0, start_index - 100)\n",
        "                context_end = min(len(cleaned_full_text), end_index + 100)\n",
        "                snippet_with_context = cleaned_full_text[context_start:context_end].strip()\n",
        "\n",
        "                keyword_docs_info.append({\n",
        "                    \"text\": snippet_with_context,\n",
        "                    \"source\": f\"關鍵詞匹配 ({pattern_type}) 全文搜索 (位置: {start_index}-{end_index})\",\n",
        "                    \"chunk_id\": f\"keyword_match_{pattern_type}_full_{matches.index(match)}\"\n",
        "                })\n",
        "                if len(keyword_docs_info) >= top_k_keyword:\n",
        "                    break\n",
        "\n",
        "            if not keyword_docs_info:\n",
        "                 st.info(\"全文關鍵詞搜索未找到匹配，回退到語義搜索。\")\n",
        "                 final_retrieved_docs = semantic_docs_info\n",
        "            else:\n",
        "                 st.info(f\"找到 {len(keyword_docs_info)} 條全文關鍵詞匹配結果。\")\n",
        "                 final_retrieved_docs = keyword_docs_info\n",
        "\n",
        "        elif search_in_semantic_results:\n",
        "            st.info(\"在語義搜索結果中執行關鍵詞搜索...\")\n",
        "            for i, doc_info in enumerate(semantic_docs_info):\n",
        "                chunk_text = doc_info[\"text\"]\n",
        "                matches = list(target_pattern.finditer(chunk_text))\n",
        "                for match in matches:\n",
        "                     snippet = match.group(0).strip()\n",
        "                     is_duplicate = any(snippet in existing_doc[\"text\"] for existing_doc in keyword_docs_info)\n",
        "                     if not is_duplicate:\n",
        "                        keyword_docs_info.append({\n",
        "                            \"text\": snippet,\n",
        "                            \"source\": f\"關鍵詞匹配 ({pattern_type}) from semantic chunk {i+1}\",\n",
        "                            \"chunk_id\": f\"keyword_match_{pattern_type}_chunk_{i}_match_{matches.index(match)}\"\n",
        "                        })\n",
        "                        if len(keyword_docs_info) >= top_k_keyword:\n",
        "                            break\n",
        "\n",
        "                if len(keyword_docs_info) >= top_k_keyword:\n",
        "                     break\n",
        "\n",
        "            if not keyword_docs_info:\n",
        "                 st.info(\"語義結果中的關鍵詞搜索未找到匹配，返回語義搜索結果。\")\n",
        "                 final_retrieved_docs = semantic_docs_info #\n",
        "            else:\n",
        "                 st.info(f\"找到 {len(keyword_docs_info)} 條語義結果中的關鍵詞匹配結果。\")\n",
        "                 final_retrieved_docs = keyword_docs_info[:]\n",
        "                 for s_doc in semantic_docs_info:\n",
        "                      is_covered_by_keyword = any(s_doc[\"text\"] in k_doc[\"text\"] for k_doc in keyword_docs_info)\n",
        "                      if not is_covered_by_keyword:\n",
        "                          is_duplicate = any(s_doc[\"chunk_id\"] == f_doc.get(\"chunk_id\") for f_doc in final_retrieved_docs)\n",
        "                          if not is_duplicate:\n",
        "                             final_retrieved_docs.append(s_doc)\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "             st.info(\"未設定關鍵詞搜索模式，返回語義搜索結果。\")\n",
        "             final_retrieved_docs = semantic_docs_info\n",
        "\n",
        "    else:\n",
        "        st.info(\"未偵測到特定關鍵詞模式，執行純語義搜索。\")\n",
        "        final_retrieved_docs = semantic_docs_info\n",
        "\n",
        "\n",
        "    return final_retrieved_docs\n",
        "\n",
        "def generate_answer_streamlit(query_text):\n",
        "    \"\"\"\n",
        "    使用檢索到的文件生成答案。\n",
        "    Assumes llm_model is available (cached).\n",
        "    \"\"\"\n",
        "    # 檢查 LLM 模型是否成功載入\n",
        "    if llm_model is None:\n",
        "        st.error(\"LLM 模型未成功初始化，無法生成答案。請檢查 API 金鑰和載入過程。\")\n",
        "        return \"無法生成答案，因為語言模型未成功初始化。\", []\n",
        "\n",
        "\n",
        "    # 步驟1：檢索相關文件 (使用優化後的混合檢索函數)\n",
        "    retrieved_docs_info = retrieve_documents_optimized(\n",
        "        query_text,\n",
        "        top_k_semantic=10, # 語義搜索取前 10 個塊\n",
        "        top_k_keyword=5 # 關鍵詞搜索最多取前 5 個匹配項\n",
        "    )\n",
        "\n",
        "    if not retrieved_docs_info:\n",
        "        return \"對不起，我沒有找到相關的金融文件來回答這個問題。\", []\n",
        "\n",
        "    # 步驟2：組合上下文\n",
        "    # 為了避免上下文過長，我們可以限制用於生成答案的文本塊數量或總字元數\n",
        "    max_context_chunks = 10\n",
        "    context_texts = [doc[\"text\"] for doc in retrieved_docs_info[:max_context_chunks]]\n",
        "    context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "    # 步驟3：格式化 Prompt\n",
        "    PROMPT_TEMPLATE = \"\"\"\n",
        "您是一位金融分析師，請根據提供的以下金融文件內容，簡潔、專業地回答問題。\n",
        "如果文件內容沒有足夠的資訊來回答問題，請明確指出「文件內容不足以回答此問題」。\n",
        "\n",
        "文件內容：\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "問題：\n",
        "{question}\n",
        "\n",
        "回答：\n",
        "\"\"\"\n",
        "    full_prompt = PROMPT_TEMPLATE.format(context=context, question=query_text)\n",
        "\n",
        "    # 步驟4：使用 Gemini API 生成答案\n",
        "    try:\n",
        "        response = llm_model.generate_content(full_prompt)\n",
        "\n",
        "        if hasattr(response, 'text'):\n",
        "            generated_answer = response.text.strip()\n",
        "        elif response.candidates:\n",
        "            generated_answer = \"\".join([part.text for part in response.candidates[0].content.parts]).strip()\n",
        "        else:\n",
        "            generated_answer = \"Gemini 模型未能生成答案，可能因安全策略或其他內部錯誤。\"\n",
        "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n",
        "                st.warning(f\"Prompt 被阻擋，原因: {response.prompt_feedback.block_reason.name}\")\n",
        "\n",
        "        return generated_answer, retrieved_docs_info\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"生成答案時發生錯誤: {e}\")\n",
        "        return \"生成答案時發生錯誤，請檢查 API 金鑰、網路連接或 Prompt 內容。\", []\n",
        "\n",
        "\n",
        "query = st.text_input(\"請輸入您的問題:\")\n",
        "\n",
        "if st.button(\"獲取答案\"):\n",
        "    if query:\n",
        "        if embedding_model is None or index_with_ocr is None or document_store_with_ocr is None or llm_model is None:\n",
        "             st.error(\"RAG 系統尚未準備好。請檢查上述載入錯誤信息並解決問題。\")\n",
        "        else:\n",
        "            with st.spinner(\"正在尋找答案...\"):\n",
        "                answer, sources = generate_answer_streamlit(query)\n",
        "\n",
        "            st.subheader(\"AI 的回答:\")\n",
        "            st.write(answer)\n",
        "\n",
        "            if sources:\n",
        "                st.subheader(f\"參考來源 ({len(sources)} 條):\")\n",
        "                max_sources_to_display = 10\n",
        "                for i, source_info in enumerate(sources[:max_sources_to_display]):\n",
        "                    st.write(f\"**來源 {i+1}:** {source_info.get('source', '未知來源')}\")\n",
        "                    source_text_preview = source_info.get('text', '無法獲取文本內容')\n",
        "                    st.write(f\"內容摘要: {source_text_preview[:200]}...\")\n",
        "                    if len(source_text_preview) > 200:\n",
        "                        st.write(\"...\")\n",
        "                if len(sources) > max_sources_to_display:\n",
        "                    st.write(f\"... 還有 {len(sources) - max_sources_to_display} 條來源未顯示。\")\n",
        "\n",
        "            else:\n",
        "                st.info(\"沒有找到直接相關的參考文件。\")\n",
        "    else:\n",
        "        st.warning(\"請輸入您的問題。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5b60fc3"
      },
      "source": [
        "## 運行包含 RAG 系統的 Streamlit 應用程式\n",
        "在 Colab 中以後台方式運行 Streamlit 應用程式，並使用 ngrok 建立公共 URL。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21db12c3"
      },
      "source": [
        "# 使用 pyngrok 建立 ngrok 通道並運行 Streamlit\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "from google.colab import userdata # 導入 userdata 模組來讀取 secrets\n",
        "import os # 導入 os 模組來檢查檔案是否存在和設定環境變數\n",
        "import uuid # 導入 uuid 模組來生成唯一的臨時檔案名\n",
        "\n",
        "# 從 Colab Secrets 中讀取 GEMINI_API_KEY\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"\\nGEMINI_API_KEY not found in Colab Secrets. Please set it up.\")\n",
        "        raise ValueError(\"GEMINI_API_KEY not set in Colab Secrets.\")\n",
        "    else:\n",
        "        print(\"GEMINI_API_KEY loaded from Secrets.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError retrieving GEMINI_API_KEY from Secrets: {e}\")\n",
        "    print(\"無法獲取 GEMINI_API_KEY，請檢查 Colab Secrets。\")\n",
        "    raise\n",
        "\n",
        "# --- 將 API 金鑰寫入臨時檔案 ---\n",
        "temp_api_key_filename = f\".gemini_api_key_{uuid.uuid4().hex}.tmp\"\n",
        "output_dir = \"/content/drive/MyDrive/FinancialRAGData\"\n",
        "temp_api_key_path = os.path.join(output_dir, temp_api_key_filename)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    with open(temp_api_key_path, \"w\") as f:\n",
        "        f.write(GEMINI_API_KEY)\n",
        "    print(f\"GEMINI_API_KEY 已寫入臨時檔案: {temp_api_key_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n錯誤：無法寫入臨時 API 金鑰檔案: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 繼續 ngrok 和 Streamlit 啟動邏輯 ---\n",
        "\n",
        "# 從 Colab Secrets 中讀取 ngrok Authtoken\n",
        "try:\n",
        "    NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        print(\"\\nNGROK_AUTH_TOKEN not found in Colab Secrets. Skipping ngrok tunnel setup.\")\n",
        "        public_url = \"NGROK_AUTH_TOKEN not set.\"\n",
        "    else:\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        print(\"ngrok Authtoken configured.\")\n",
        "\n",
        "        # 檢查 app.py 檔案是否存在\n",
        "        if not os.path.exists(\"app.py\"):\n",
        "            print(\"\\nError: app.py not found. Please ensure you have created the app.py file.\")\n",
        "            public_url = \"app.py not found.\"\n",
        "        else:\n",
        "            # 啟動 Streamlit 應用程式在背景運行\n",
        "            print(\"Starting Streamlit app in background...\")\n",
        "            !nohup streamlit run app.py --server.port 8501 > streamlit.log 2>&1 &\n",
        "\n",
        "            # 等待 Streamlit 服務器啟動\n",
        "            print(\"Waiting for Streamlit server to start...\")\n",
        "            time.sleep(20) # 顯著增加等待時間，給 Streamlit 更多時間啟動和載入 RAG 組件\n",
        "\n",
        "            # 建立 ngrok http 通道到 Streamlit 運行的端口 (8501)\n",
        "            print(\"Attempting to establish ngrok tunnel...\")\n",
        "            try:\n",
        "                public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "                print(f\"\\nStreamlit 應用程式正在運行，請透過以下 URL 訪問:\")\n",
        "                print(public_url)\n",
        "\n",
        "                print(\"\\nKeeping the notebook cell alive. Press stop to terminate.\")\n",
        "                time.sleep(3600)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError establishing ngrok tunnel: {e}\")\n",
        "                public_url = \"Failed to establish ngrok tunnel. See error message above.\"\n",
        "\n",
        "except Exception as e:\n",
        "    # 如果在設置 GEMINI_API_KEY 或 ngrok Authtoken 階段出錯，或者在啟動Streamlit/ngrok時出錯，這裡會捕獲並打印\n",
        "    print(f\"\\nAn error occurred during setup or Streamlit/ngrok launch: {e}\")\n",
        "    if 'public_url' not in locals():\n",
        "         public_url = \"An unexpected error occurred during launch.\"\n",
        "\n",
        "finally:\n",
        "    # --- 嘗試清理臨時檔案 ---\n",
        "    if 'temp_api_key_path' in locals() and os.path.exists(temp_api_key_path):\n",
        "        try:\n",
        "            os.remove(temp_api_key_path)\n",
        "            print(f\"臨時 API 金鑰檔案已清理: {temp_api_key_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n警告：無法清理臨時 API 金鑰檔案: {e}\")\n",
        "\n",
        "\n",
        "print(f\"\\nFinal public_url status: {public_url}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型選擇與訓練\n",
        "本專案使用了嵌入模型和向量資料庫來實現高效的文本檢索。\n",
        "\n",
        "1.  **嵌入模型 (Embedding Model)**:\n",
        "    *   使用 `sentence-transformers` 函式庫載入預訓練的嵌入模型。\n",
        "    *   筆記本中嘗試並最終用於 Streamlit 應用的模型是 `bert-base-multilingual-cased`。這個模型支援多國語言，對於處理包含中文的金融文本是合適的選擇。\n",
        "    *   `get_embedding` 函式用於將文本塊或查詢轉換為高維度向量。\n",
        "\n",
        "2.  **向量資料庫 (FAISS Index)**:\n",
        "    *   使用 **FAISS (Facebook AI Similarity Search)** 庫來建立和管理文本塊的嵌入向量索引。\n",
        "    *   選擇了 `IndexFlatL2` 索引類型，使用 L2 距離（歐幾里得距離）來衡量向量之間的相似性。\n",
        "    *   將所有文本塊的嵌入向量添加到 FAISS 索引中。\n",
        "    *   建立了一個 `document_store` 字典，用於儲存每個索引對應的原始文本內容和元數據。\n",
        "    *   將 FAISS 索引 (`tsmc_financial_docs.faiss`) 和 `document_store` (`tsmc_document_store.json`) 保存到 Google Drive 中，以便 Streamlit 應用程式載入。\n",
        "\n",
        "## 結果與分析\n",
        "通過整合 OCR 功能，本專案旨在提高 RAG 系統在處理包含圖像化文字的 PDF 檔案時的檢索準確性，進而提升問答效果。\n",
        "\n",
        "*   **測試問題**: 筆記本中使用了多個測試問題來評估系統，包括：\n",
        "    *   「台積電114年第一季的合併營收是多少？」\n",
        "    *   「台積電的會計師核閱報告是由哪家會計師事務所出具的？」\n",
        "    *   「台積電在大陸的投資資訊有什麼？」\n",
        "    *   「台積電的股票代碼是什麼？」\n",
        "    *   「蘋果公司最近推出了什麼新產品？」 (用於測試系統處理無關問題的能力)\n",
        "\n",
        "*   **檢索策略改進**: 實現了混合檢索策略 `retrieve_documents_optimized`，結合了語義搜索（使用嵌入向量和 FAISS）和針對特定問題類型（如營收、股票代碼、會計師事務所）的關鍵詞匹配。關鍵詞匹配優先，這對於精確檢索特定事實資訊（如準確的營收數字或股票代碼）尤其有效，即使在語義搜索效果不佳的情況下也能提供相關依據。\n",
        "\n",
        "*   **回答生成**: 使用 Google 的 **Gemini 1.5 Flash** 模型作為語言模型 (`llm_model`)。將檢索到的最相關文本塊作為上下文，與用戶查詢一起格式化成 Prompt 模板，提交給 Gemini 模型生成答案。Prompt 模板指示模型扮演金融分析師的角色，並在資訊不足時明確說明。\n",
        "\n",
        "*   **效果分析**: 整合 OCR 後，系統能夠從原本可能無法提取文字的圖片頁面（如包含表格的頁面，筆記本輸出顯示頁面 3-10 進行了 OCR）中獲取資訊。這對於包含關鍵財務數據的表格尤為重要。混合檢索策略進一步提高了對特定問題的檢索精確度。測試結果應顯示系統能夠根據文件內容正確回答大多數問題，並在沒有相關資訊時給出適當的回應。與未整合 OCR 的系統相比，預期在處理包含大量圖片化數據的財報時表現會有顯著提升。\n",
        "\n",
        "## 討論模型在實際應用中的潛力\n",
        "這個整合 OCR 的 RAG 系統在金融領域，特別是處理 PDF 格式的財務報告方面具有顯著潛力：\n",
        "\n",
        "*   **優勢**:\n",
        "    *   能夠處理包含圖片和表格的複雜 PDF 格式，克服傳統文本提取的限制。\n",
        "    *   提供基於具體文檔內容的回答，減少大型語言模型幻覺的可能性。\n",
        "    *   混合檢索策略結合語義和關鍵詞匹配，提高了檢索的靈活性和準確性。\n",
        "    *   提供參考來源，增強答案的可信度。\n",
        "    *   Streamlit 介面使得系統易於使用和展示。\n",
        "\n",
        "*   **潛在限制**:\n",
        "    *   OCR 的準確性受圖片品質、字體、佈局複雜度影響，錯誤的 OCR 結果會直接影響下游檢索和生成。\n",
        "    *   大型文件可能導致 Chunking 和嵌入過程計算成本較高，且單個 Prompt 的上下文長度有限。\n",
        "    *   關鍵詞搜索的模式需要針對特定資訊手工編寫和調整。\n",
        "    *   對於需要跨越多個文本塊才能獲取的複雜資訊，RAG 系統的表現可能受限。\n",
        "    *   依賴外部 API (如 Gemini)，存在成本和穩定性問題。\n",
        "\n",
        "*   **未來改進方向**:\n",
        "    *   使用更先進的 OCR 模型或專門針對表格結構的提取工具。\n",
        "    *   探索更有效的 Chunking 策略，例如基於語義段落或文件結構的切割。\n",
        "    *   嘗試不同或更高性能的嵌入模型，特別是針對金融領域優化的模型。\n",
        "    *   研究更複雜的檢索策略，例如重新排序 (Re-ranking) 或基於圖的檢索。\n",
        "    *   整合更多金融數據源或歷史財報，擴展知識庫。\n",
        "    *   實現更細粒度的來源引用，精確指出答案來自文檔的哪一部分。\n",
        "\n",
        "## 結論與未來工作\n",
        "本專案成功地將 OCR 技術整合到基於 PyMuPDF、Sentence-Transformers、FAISS 和 Gemini 的 RAG 系統中，提高了系統處理包含圖片化文字的 PDF 財務報告的能力。建立的 Streamlit 應用程式提供了一個可互動的介面來測試和展示系統。\n",
        "\n",
        "未來工作應著重於進一步提升 OCR 準確性、優化文本處理和檢索策略，以及探索更豐富的數據源，從而構建一個更強大的金融 RAG 系統，更好地服務於金融分析和資訊查詢需求。"
      ],
      "metadata": {
        "id": "S6Sl11oiUGY8"
      }
    }
  ]
}